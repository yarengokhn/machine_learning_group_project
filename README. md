# Code-to-Text Summarization (Seq2Seq)

This project implements an end-to-end **code-to-text summarization system** using a custom **Seq2Seq encoder–decoder neural network**.  
The model learns to generate short natural language summaries from Python source code.

---

## Project Overview

The goal of this project is to demonstrate the complete machine learning pipeline:

- dataset loading
- preprocessing and tokenization
- model training
- evaluation
- inference on unseen code samples

The system is trained and evaluated on a subset of the **CodeXGLUE code-to-text dataset**.

---

## Dataset

- **Source**: CodeXGLUE – code-to-text
- **Programming language**: Python
- **Subset size**: 200 samples (for faster experimentation)
- **Input**: Python functions / methods
- **Target**: Docstring-style natural language summaries

---

## Model Architecture

- **Type**: Encoder–Decoder (Seq2Seq)
- **Encoder**: HybridEncoder
- **Decoder**: HybridDecoder
- **Embedding dimension**: 128
- **Hidden dimension**: 256
- **Vocabulary**: Shared tokenizer vocabulary
- **Loss function**: CrossEntropyLoss (padding tokens ignored)
- **Optimizer**: AdamW

---

## Training Setup

- **Epochs**: 5
- **Batch size**: 32
- **Device**: CPU
- **Frameworks**:
  - PyTorch
  - HuggingFace Datasets & Transformers

---

## Results

- **Training loss**: ~4.64
- **Test loss**: **4.7061**

The relatively high loss is expected due to:
- small dataset size,
- limited number of epochs,
- training from scratch without pretrained weights.

---

## Inference Example

**Input code (preview):**
```python
def settext(self, text, cls='current'):
    """set the text for this element."""
